---
title: CS4246 notes
tags: Module, CS4246
---

# CS4246 notes

---

## Chapter 2: Classical planning

### Problem solving

* Problem consists of 
    * initial state and states
    * actions
    * transition model
    * goal test
    * path cost
* Uniform cost search (Dijkstra's algorithm)
    * repeatedly expand lowest cost node in the frontier
* A* search
    * UCS with $f(n)=g(n)+h(n)$
    * $g(n)$ cost used to reach node $n$
    * $h(n)$ heuristic: estimated cost from $n$ to goal
        * admissible (optimal for tree search): never overestimate cost to goal
        * consistent (optimal for graph search): $h(n) \leq c(n,a,n') + h(n')$ - along the path, estimation from $h$ gets closer to real cost
    * Relaxation: optimal cost for a relaxed problem is usually an admissible heuristic
* Large state space
    * BFS-like algo (e.g. A*) in the worst case will store all states in the frontier
    * DFS uses $O(m)$ memory where $m$ is depth of search
    * iterative deepening search / A* can make search feasible
* Domain independent heuristics

### STRIPS and PDDL

* **Factored representation**: opposite of **atomic representation**, we use a set of (variable/value) pairs. For example, in 8 puzzle problem, we use `At(t1, p1)=true`, `At(t2, p2)=false` to denote whether a tile is at a position
* **Fluents**: state variables (attributes that can change through time) are called **fluents**. In the previous example, `At(t2, p2)` is one of the fluents

#### Logic notation

* **term**: a variable, constant or function
* **atomic sentence/atom**: a predicate symbol (optionally followed by a parenthesized list of terms) e.g. Brother(John, x), Rich - these evaluates to true or false
* **literal**: an atom or its negation
* **ground literal**: a literal with no variables
* **sentence/formula**: atoms combined with quantifiers ($\forall, \exists$), logical connectives ($\land, \lor, \lnot$) and equality symbol
* **substitution**: replace variables by constants
* **unifier**: a substitution to variables that makes two sentences identical
* **most general unifier**: for every pair of unifiable sentences, there exists a unique MGU up to renaming and substitution of variables

#### STRIPS (Stanford research institute problem solver)

* State: a **conjunction** of **positive** literals that are **ground and function-free**
    * **Database semantics** are used: literals not mentioned are false (close world assumption)
* Goal: a partially specified state represented as a **conjunction** of **positive ground** literals
    * a state $s$ satisfies a goal $g$ if it contains all the literals in $g$
* Action schema:
    * Precondition: a conjunction of function-free positive literals
    * Effect: a conjunction of function-free positive (add) or negative (delete) literals
    * Result(s,a)=$(s-Del(a))\cup Add(a)$, first remove, then add
* A set of action schemas defines a **planning domain**

#### PDDL

* Less restrictive than STRIPS
    * allows inequality

### Planning as state-space search

#### Forward search

Start from initial state, take actions until reaches goal

* State space can be large - exponential to # of state variables
* Action space can be large
* Forward search is hopeless without good heuristics
    * but with one can be feasible

#### Backward search (relevant-state search, regression)

* **relevant** because only consider actions that are relevant to the goal/current state
* State vs description, with $n$ ground fluents
    * there are $2^n$ states as each fluent is either `true` or `false`
    * there are $3^n$ descriptions as each fluent can be `true` or `false` or `unknown`
        * a description represents a set of states
* Backward search: regress from a description to a predecessor description
    * goal $g$, action $a$, predecessor description $g'$: $$g'=(g-Add(a))\cup Precond(a)$$
        * note that delete list is not used, but if delete list doesn't agree with the goal then this action is not relevant
* Relevant action
    * add list: at least one of the effect unify with an element in the current goal
    * delete list: must not have any effect that negate an element of the current goal
* Pros: lower branching factor
* Cons: uses sets of states, difficult to have good heuristics

#### Heuristics for planning

* Ignore preconditions
* Ignore delete list
    * allows monotonic progress towards STRIPS goal
* Problem decomposition
    * divide into subgoals, solve subgoals, then combine
    * divide set of goals into disjoint subsets
        * if each subproblem uses admissible heuristic, sum is admissible as well

### SATPlan

CNF (conjunctive normal form) formulas
* literal = a variable or its negation
* clause = disjunction of literals
* CNF formula = conjunction of clauses (AND of ORs)

Translate STRIPS to SAT
* Propositionalize the actions: replace each action schema with a set of grounded actions by substituting constants for each variable
    * action: `Fly(p,from,to)`
    * grounded: `Fly(P1,SFO,JFK)`, `Fly(P2,JFK,SFO)`
* Define initial state
    * assert for every fluent in initial state
    * assert the negation for every fluent not in initial state
* Propositionalize the goal: construct a disjunction over all possible ground conjunctions
    * goal: $On(A,x) \land Block(x)$
    * grounded: ![](https://i.imgur.com/FcjC2Jy.png)
* Add **successor-state axioms**
    * for each fluent $F$, add
    * $F^{t+1} \iff ActionCauseF^t \lor (F^t \land \lnot ActionCauseNotF^t)$
    * with $m$ actions and $n$ fluents, we need $O(mn)$ frame axioms but only $O(n)$ successor-state axioms
* Add **precondition axioms**
    * for each ground action $A$, add
    * $A^t \rightarrow Pre(A)^t$ - if action $A$ is taken at time $t$, then its preconditions must be true at time $t$
* Add **action exclusion axioms**
    * only one action is allowed at one time step
    * for every pair of actions $A_i^t$ and $A_j^t$, add $\lnot A_i^t \lor \lnot A_j^t$
    * if parallel actions allowed, add exclusion for only interfering actions
* number of steps is unknown, try $T$ up to $T_{max}$

### Complexity of planning

* PlanSAT: if a plan exists
* Bounded PlanSAT: if a plan of no longer than $k$ exists
* Both PlanSAT and bounded PlanSAT are in PSPACE
    * NP-hard, even if negative effects disallowed
    * PlanSAT is P if negative preconds also disallowed

## Chapter 3: Decision theory

### Utility theory

#### Axioms

* Axiom 1: orderability or completeness
    * given two outcomes $A,B\in S$, exactly one of A>B, B>A, A~B holds
* Axiom 2: transitivity
    * if A>B and B>C, then A>C
    * similarly for ~
* Consequences
    * exists a function $U$ s.t. $U(A)>U(B)\iff A>B$ and $U(A)=U(B)\iff A\sim B$
    * function $U$ is not unique, any monotonically increasing transformation preserves the property
* Rationality
    * have **complete** and **transitive** preferences

#### Lotteries

A lottery $L$ with outcomes $S_1,\cdots,S_n$ of probabilities $p_1,\cdots,p_n$ is denoted as $$L=[p_1,S_1;p_2,S_2;\cdots;p_n,S_n]$$

Axioms of utility theory
* orderability: given two lotteries, an agent has one of three possibilities: prefer A, prefer B, indifferent
* transitivity
* continuity: $A\succ B\succ C \rightarrow \exists p s.t. [p,A;1-p,C]~B$
* substitutability: $A\sim B \rightarrow [p,A;1-p,C] \sim [p,B;1-p,C]$
    * also holds if replace $\sim$ with $\succ$

Implications of the axioms:
* monotonicity: $A\succ B \rightarrow (p>q \iff [p,A;1-p,B]\succ [q,A;1-q,B])$
* decomposibility: compound lotteries can be reduced to simpler ones
    * ![](https://i.imgur.com/05NzGRF.png)

von Neumann and Morgenstern theorem
* existence of utility function
    * similar to state version, holds if agent's preference obey axioms of utility theory above
* expected utility of a lottery
    * $U([p_1,S_1;p_2,S_2;\cdots;p_n,S_n])=\sum_i p_iU(S_i)$

Principle of maximum expected utility (MEU)
* an agent can act rationally **only** by choosing an action that **maximizes the expected utility**
    * consequence: $U$ is uniquely determined up to $aU(S)+b$ with $a>0$
    * different from deterministic env, with lottery, only positive affine transformation works

### Utility functions

* Preference elicitation
    * fix utilities of two outcomes to establish scale
        * e.g. fix worst outcome and best outcome
        * for normalized utilities, $u_{best}=1$ and $u_{worst}=0$
    * to get $U(S)$ of prize S
        * ask agent to choose between $S$ and $[p,u_{best};1-p,u_{worst}]$
        * adjust $p$ until agent is indifferent
    * impossible to elicit preference of a group
* Utility of money
    * Expected monetary value (EMV) != utility
    * ![](https://i.imgur.com/NqfiC0X.png)
    * ![](https://i.imgur.com/j2i0eUT.png) concave utility function like so is **risk-averse**
        * concave -> risk-averse: $U(L) < U(S_{EMV}(L))$
        * convex -> risk-seeking: $U(L) > U(S_{EMV}(L))$
    * certainty equivalent
    * risk premium: diff between EMV and its certainty equivalent



## Chapter 4: MDP

### Markov decision process

A sequential decision problem for a **fully observable, stochastic environment** with a **Markkovian transition** and **additive rewards** is called a Markov decision process (MDP).

* Transition model
    * $P(s'|s,a)$ probability of transitioning to state $s'$ if we take action $a$ at state $s$
    * **Markovian assumption**: probability doesn't depend on history of earlier states
* Reward function
    * $R(s)$
    * $R(s,a,s')$
* Policy $\pi(s)$
    * a function from state to action
    * Expected return: expected utility of possible state sequences generated by the policy
    * Optimal policy $\pi^*$: with the highest expected utility

### Utility function

* Finite horizon
    * exists a fixed time $N$ after which nothing matters
    * $U(s_0) = R(s_0) + R(s_1) + \cdots + R(s_N)$
    * Optimal policy is **nonstationary** (i.e. time dependent, $\pi^*$ is a function of both state and current time step)
* Infinite horizon
    * optimal policy is stationary
    * discounted rewards: $U([s_0, s_1, \cdots]) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots$ where discount factor $\gamma \in (0,1)$
        * if rewards are bounded by $R_max$, then discounted reward is finite: $U \leq R_{max} / (1-\gamma)$
    * Expected utility of policy $\pi$ starting from state $s$ is $$U^\pi(s)=E[\sum_t^\infty \gamma^t R(s_t)]$$
    * Optimal policy $\pi^*_s = argmax_\pi U^\pi(s)$
    * Value function: the value (utility) of an optimal policy: $$V(s)=U^{\pi^*}(s)$$
    * Given the value function, optimal policy can be obtained by $$\pi^*(s) = argmax_{a\in A(s)} P(s'|a,s)V(s')$$

### Value iteration

* Value function = value under **optimal policy**: $V(s)=U^{\pi^*}(s)$
    * given value function, optimal policy can be obtained by $\pi(s)=argmax_a \sum_{s'}P(s'|s,a)V(s')$
* The value function of optimal stationary policy satisfies **Bellman equation** $$V(s)=R(s)+\gamma \max_{a\in A(s)} \sum_{s'} P(s'|s,a)V(s')$$
* **Value iteration** algorithm does the **Bellman update** repeatedly: $$U_{t+1}(s) \leftarrow R(s)+\gamma \max_{a\in A(s)} \sum_{s'} P(s'|s,a)U_t(s')$$
    * termination condition: $||U_{t+1}-U_t||\leq \epsilon (1-\gamma)/\gamma$
        * which implies $||U_{t+1}-V||\leq \epsilon$
    * it converges to the **unique** value function (proof: tutorial week 7)
    * $||U_t - V||$ goes to zero exponentially - after $N$ iterations, the error is at most $\gamma^N R_{max} / (1-\gamma)$ (assuming $U_0=0$, since value function is upper bounded by $R_{max} / (1-\gamma)$)
    * number of iterations required to achieve less than $\epsilon$ error: $$N = \lceil \log(R_\max / \epsilon(1-\gamma) / \log(1/\gamma)) \rceil$$
* Value iteration has complexity $O(N|S||A|)$, not feasible for large state space
    * $N$ is max number of iterations calculated above

### Policy iteration

* Repeat
    * policy evaluation: calculate $U_i = U^{\pi_i}$
    * policy improvement: calculate new policy $\pi_{i+1}$ using one step look-ahead based on $U_i$
* terminate when no change in policy - must happen as there are only a finite number of policies
* **Essentially solving a linear system**: $$U^\pi(s)=R(s)+\gamma \sum_{s'} P(s'|s,\pi(s))U^\pi(s')$$
    * no $\max$ operator as in Bellman equation
    * $O(|S|^3)$ complexity using Gaussian elimination
    * for large $|S|$, can use iterative method
* Policy improvement theorem
    * let $\pi$, $\pi'$ be such that for all $s \in S$, $$Q^\pi(s, \pi'(s)) \geq U^\pi(s)$$, then $U^{\pi'}(s) \geq U^\pi(s)$ for all $s\in S$
* Generalized policy iteration
    * modified PI: only do $k$ iterations instead of until converge
    * asynchronous PI: only pick a subset of states to do policy evaluation/improvement
    * generalized PI: update value according to policy, improve policy using values - VI, PI, API are all special cases of GPI

### Online search

#### Curse of dimensionality

* Both value and policy iteration iterates through all **states**, which grows exponentially w.r.t number of state variables
* Solution 1: function approximation
* Solution 2: online search with sampling

![](https://i.imgur.com/LA6nnas.png)

* Circle: action node, Black box: observation node
* With depth $D$, the size of search tree is $|A|^D |S|^D$
    * sample a fixed number $k$ of observation: becomes $|A|^D k^D$ - no curse of dimensionality, but still exponential to $D$ - curse of history

#### Fix curse of dimensionality
* Rollout
    * Estimate $Q(s,a)$ starting from state $s$ by taking action $a$ first, then follow the policy $\pi$. After reaching the terminal node, **backup** the return.
    * Perform the simulation many times, choose action with highest average return
* Monte Carlo Tree Search: repeatedly run trials from the root, where a trial
    * repeatedly **select** a node for next level until
        * target depth reached
        * selected node is not visited - create one new node, run simulation using rollout policy till required depth
        * node selection
            * apply an action $a$, then observe the outcome from $P(s'|s,a')$
            * if selection is deterministic wrt state, then degraded to a rollout policy
            * Upper confidence tree: balance exploration and exploitation with $$\pi_{UCT}(n)=argmax_a (\hat{Q}(n,a) + c\sqrt{\frac{\ln(N(n))}{N(n,a)}})$$
                * $N(n)$ and $N(n,a)$ counts number of occurrences
                * UCT will eventually converge to optimal policy with enough trials
    * backup the outcomes

## Chapter 5: Reinforcement learning

### Model-based RL

* Prediction and control
    * Prediction: for a fixed policy $\pi$, predict how good a state is by evaluating the value function $V$ or action-value function $Q$
    * Control: learn the policy $\pi$
* Model-based prediction
    * learn the model
        * transition probabilities $P(s'|s,\pi (s))$
        * reward function $R(s)$
* Adaptive Dynamic Programming (ADP) agent
    * Transition probability: max likelihood estimate
        * Counters: $N_{sa}$ for $(s,a)$ pair, $N_{s'|sa}$ for $(s',s,a)$ pair
        * $P(s'|s,a) = N_{s'|sa}(s',s,a) / N_{sa}(s,a)$
    * Value function
        * Recall: policy evaluation is to solve the linear equation $U^\pi(s)=R(s)+\gamma \sum_{s'} P(s'|s,U^\pi(s))U^\pi(s')$, for $n$ states, it takes $O(n^3)$ time
    * ![](https://i.imgur.com/cyMCqKj.png)
* Model-based control (**greedy** ADP agent)
    * Replace `Policy-Evaluation` with $\pi$ = `Policy-Iteration(mdp)`
    * it's a **greedy** agent, and sometimes never tries the better policy sufficiently after finding a suboptimal one
* $\epsilon$-greedy exploration
    * choose the greedy action with prob $1-\epsilon$ (exploitation) and a random action with prob $\epsilon$ (exploration)
* Greedy in the limit of infinite exploration (GLIE)
    * To converge to an optimal policy, initially try each action in each state an unbounded number of times to avoid a finite prob of missing an optimal action
    * Eventually becomes greedy so that it's optimal w.r.t. the true model
    * GILE: do $\epsilon$-greedy exploration with $\epsilon = 1/t$

### Model-free RL

* Model-free: learn the value function **without** learning the transition and reward functions
* Model-free prediction
    * $U^\pi(s)=E[\sum_{t=0}^\infty \gamma^tR(S_t)|S_0=s]$ (run infinite trials starting from $s$ and average the total reward from that state onwards)
* Monte Carlo learning (direct utility estimation)
    * keep a running average for each state in a table, with infinitely many trials, sample average will converge to expected value
    * Assume state $s$ is encountered $k$ times, denote the ith return as $G_i(s)$, $$\begin{align}U_k(s)&=\frac{1}{k}\sum_{i=1}^k G_i(s)\\&=\frac{1}{k}(G_k(s) + \sum_{i=1}^{k-1} G_i(s))\\&=\frac{1}{k}(G_k(s) + (k-1)U_{k-1}(s))\\&=U_{k-1}(s)+\frac{1}{k}(G_k(s)-U_{k-1}(s))\end{align}$$
    * $G_k(s)-U_{k-1}$ is the **prediction error** for kth return
    * Pros: simple, unbiased estimate (supervised learning with MSE)
    * Cons
        * need to wait till the end of episode
        * variance can be high
* Model-free learning
    * Learn action-utility function $Q(s,a)$ instead
    * Policy: $\pi(s)=argmax_a Q(s,a)$
    * Utility: $U(s)=\max_a Q(s,a)$
    * Bellman equation: $Q(s,a) = R(s) + \gamma \sum_{s'} P(s'|s,a)\max_{a'}Q(s',a')$
        * if using MC learning for Q-function, no need for a model to learn and take action
        * if using ADP, need to learn $R$ and $P$ first, then solve $Q$ using iterative method, then generate action
* MC control with GPI (generalized policy iteration)
    * ![](https://i.imgur.com/laHqPYi.png)
    * repeat: MC policy evaluation -> use greedy action to improve the policy
* GLIE $\epsilon$-greedy MC control
    * sample kth trial using current Q-function + $\epsilon$-greedy exploration with $\epsilon=1/k$
    * we get state, action, reward sequence: $S_1,A_1,R_1,\cdots,S_T$
    * for each $S_t,A_t$ in the trial
        * $N(S_t,A_t) = N(S_t,A_t) + 1$
        * $Q(S_t,A_t) = Q(S_t,A_t) + \frac{1}{N(S_t,A_t)}(G_t-Q(S_t,A_t))$
* Temporal-difference learning
    * For a transition from state $s$ to $s'$, TD learning does: $$U^\pi(s) = U^\pi(s) + \alpha (R(s)+\gamma U^\pi(s') - U^\pi(s))$$
        * $R(s)+\gamma U^\pi(s')$ is TD target (ground truth)
        * $U^\pi(s)$ is current estimate, `target-current=TD error`
        * $\alpha$ is the learning rate
    * TD learning converges if $\alpha$ decreases with number of visited states
    * still model-free as only observed values are used
* TD vs. MC
    * TD learning is **online** - update with every step; MC needs a complete episode
    * TD target only depends on current rewards, MC target is the sum of all future rewards
        * TD target - lower variance, biased
        * MC target - higher variance, unbiased
    * TD usually converges faster than MC
* n-step TD
    * TD uses 1-step return $R(s)+\gamma U^\pi(s')$
    * Use $G_{t:t+n}=R_t+\gamma R_{t+1} + \cdots + \gamma^{n-1}R_{t+n-1} + \gamma^n \hat{U}(S_{t+n})$ as the target
    * when $n=\infty$ it becomes Monte Carlo
* TD($\lambda$)
    * weighted sum of n-step returns, n-th step is weighted by $\gamma^{n-1}$
    * converges to TD as $\gamma \rightarrow 0$
    * converges to MC as $\gamma \rightarrow 1$
    * ![](https://i.imgur.com/KbHZ82X.png)
* ADP vs. TD
    * ADP is model-based, TD is model-free
    * ADP requires less data from the real world
    * TD requires less computation
* TD control
    * SARSA: $Q(s,a)=Q(s,a)+\alpha (R(s)+\gamma Q(s',a') - Q(s,a))$
        * $a'$ is the action taken at $s'$, chosen by $\epsilon$-greedy
        * on-policy as it uses $\epsilon$-greedy
    * Q-learning: $Q(s,a)=Q(s,a)+\alpha (R(s)+\gamma max_{a'}Q(s',a')-Q(s,a))$
        * $R(s)$ and $s'$ are observed
        * off-policy, or *behavior policy*
    * With a fixed $\epsilon$ , SARSA is not guaranteed to converge while Q-learning does

### Function approximation

Use function approximation to represent utility and Q-functions
* common: linear function of features $U_\theta(s) = \theta_1f_1(s) + \theta_2f_2(s) + \cdots$
* also allows generalization from small number of observed states to the entire state space

Learning the function approximation
* MC learning
    * training examples: (state, utility) pairs
    * supervised training, traditional linear regression
* Online learning
    * loss function: $L(s)=(u(s)-U_\theta(s))^2/2$
        * $U_\theta(s)$ is our estimate
        * $u(s)$ is observed utility (for MC)/TD target (for TD)
    * for parameter $\theta_i$, update equation is $$\theta_i = \theta_i+\alpha(u(s)-U_\theta(s))\frac{\partial U_\theta(s)}{\partial \theta_i}$$
* Online learning for temporal difference (semi-gradient methods)
    * for TD: $$\theta_i = \theta_i+\alpha(R(s)+\gamma U_\theta(s')-U_\theta(s))\frac{\partial U_\theta(s)}{\partial \theta_i}$$
    * for Q-learning: $$\theta_i = \theta_i+\alpha(R(s)+\gamma \max_{a'}Q_\theta(s',a')-Q_\theta(s))\frac{\partial Q_\theta(s)}{\partial \theta_i}$$
    * for SARSA: $$\theta_i = \theta_i+\alpha(R(s)+\gamma U_\theta(s',a')-U_\theta(s))\frac{\partial U_\theta(s)}{\partial \theta_i}$$
        * where $a'$ is still selected $\epsilon$-greedy
    * for passive TD learning, update rule converges for linear function when used on-policy

#### Deep Q-learning

* Experience replay with fixed Q-targets
    * replay buffer: store $(s_t,a_t,r_{t+1},s_{t+1})$ in buffer $D$ for recent transitions
        * sample a mini-batch $(s,a,r,s')$ from $D$
        * set target to $r+\gamma \max_{a'}Q(s',a',\theta^-)$
        * do gradient step on minibatch squared loss wrt $\theta$
    * fixed Q-target
        * set $\theta^-$ to $\theta$ every $C$ steps

### Policy search

* Policy $\pi$ is a mapping from states to actions - we can treat it as a function, that can be parameterized by $\theta$
    * for example: $\pi(s) = argmax_a \hat{Q}_\theta(a,s)$ implies that such a policy can be described by $\theta$
* Policy search adjusts $\theta$ to improve the policy
    * Policy search vs. Q-learning
        * if $Q^*$ is optimal, using $Q^* / 10$ for our policy is just as optimal
        * policy search accepts $Q^* / 10$ as well, while Q-learning approximates $Q^*$
        * therefore for discrete actions, do classification (classify each state to action) instead of regression may make training easier
* Stochastic policy $\pi_\theta(s,a)$ = prob of taking action $a$ in state $s$ under policy $\pi$
    * makes policy a continuous function w.r.t $\theta$ - easier for gradient based search
    * discrete -> continuous: softmax function $$\pi_\theta(s,a)=e^{\hat{Q}_\theta(s,a)}/\sum_{a'}\hat{Q}_\theta(s,a')$$
        * if $\hat{Q}_\theta$ is differentiable then $\pi_\theta(s,a)$ is also differentiable
* Policy value $U(\theta)$
    * optimization criterion for $\theta$
    * for stochastic environment and/or policy $\pi_\theta(s,a)$, simply run one or several trials using $\pi_\theta$ to estimate $\nabla_\theta U(\theta)$
* Gradient of policy value $\nabla U(\theta)$
    * non-sequential case
        * sample version
    * sequential case
    * REINFORCE
    * policy gradient theorem
* Variance reduction using a baseline
* Actor critic

## Chapter 6: POMDP

### Partial observability

Instead of knowing the **state**, we perceive some **evidence** $e$ that's helpful for inferring state $s$.

* Same as MDP: states, actions, transition, reward
* Addition: **observation/sensor model**: $P(e|s)$, probability of observing evidence $e$ in state $s$
* **belief (state)**: probability distribution over the possible states - $b(s) = P(s|e)$
    * tracking belief - filtering: current belief is $b(s)$, agent does action $a$ and receives evidence $e$, then $$b'(s')=\alpha P(e|s') \sum_s P(s'|s,a)b(s)$$
        * $\alpha$ is normalizing constant
        * from Bayes' rule: $b'(s')=P(s'|e)\propto P(e|s')P(s')$, where $P(s')=\sum_s P(s'|s,a)b(s)$
* Optimal policy in POMDP is a mapping from **belief** to action
    * belief is the state in POMDP
* POMDP agent step
    * given current belief $b$, pick action $\pi^*(b)$
    * receive observation $e$
    * update the belief

### Belief space MDP

* Reward function in the belief space: $$\rho(b)=\sum_s b(s)R(s)$$
* Transition function $P(b'|b,a)$ can be derived from POMDP - using *FORWARD* formula
* $\rho(b)$ and $P(b'|b,a)$ defines an observable MDP in belief space (i.e. state space is now belief space), optimal policy for this MDP is also optimal for POMDP

**Problem**: belief space is continuous - value/policy iteration doesn't work

### Value iteration for POMDP

Conditional plan:

![](https://i.imgur.com/AfJVtpl.png)

For finite actions $|A|$, observations $|E|$ and depth $d$, there are $$|A|^{O(|E|)^{d-1}}$$ conditional plans (which is also finite).

* $\alpha_p(s)$ is the utility of executing plan $p$ from state $s$
* $\sum_s b(s)\alpha_p(s)$ is the utility of executing plan $p$ from belief $b$
    * vectorized: $\vec{b} \cdot \alpha_p$
    * for fixed conditional plan $p$, value function $U_p(b)$ is a **linear function** of belief $b$
* Optimal policy is to choose the plan with max utility: $U(b)=\max_p b\cdot \alpha_p$

Value iteration:

![](https://i.imgur.com/j2ww316.png)

### Scaling POMDP solvers

* Exact POMDP is very hard, approximate solvers can scale **moderately**
* Online search scales better
* Need to scale two things
    * belief tracking
    * planning

#### Belief tracking

* $b'(s')=\alpha P(e|s') \sum_s P(s'|s,a)b(s)$ needs to go through the entire state space - which grows exponentially w.r.t. number of state variables.
* Can use various approximate inference algorithms
    * e.g. *particle filters*

#### Online search

![](https://i.imgur.com/3HQ8DFL.png)

* Size of search tree of depth $d$: $O(|A|^d|E|^d)$
* POMCP - UCT on POMDP
    * instead of describing belief using a probability distribution
    * belief in POMCP is action-observation **history** - given the same initial belief, history is equivalent, and we fix the search depth so history is not too long
    * POMCP samples a state at root from initial belief, then run simulation
        * only need $p(s'|s,a)$ and $p(e|s')$ for simulation
        * avoid constructing/filtering beliefs
* DESPOT
    * construct $k$ different search tree, each with a root sampled from the initial belief
    * at action node: try all actions
    * at observation node: sample **a single** observation
        * sufficient if a **small good** policy exists
    * size for each of the $k$ trees: originally $|A|^d|E|^d$, now $|A|^d$ as we don't sample all observations
    * use heuristics to do anytime search

### OpenAI Five

* Handle partial observability
    * model-free reinforcement learning: LSTM, a type of RNN
* Exploration
    * start with self-playing from random weights - a natural curriculum
    * reward shaping
* Learning to coordinate
    * reward  a combination of individual reward and average team reward


## Chapter 7: Game Theory

### Single move games

Three components:
* players
* actions
* payoff function: mapping from players' actions to the utility of each player
    * can be represented with a matrix

Strategy
* pure strategy: deterministic action
* mixed strategy: randomized
* strategy profile: assignment of strategy to each player
* solution: a strategy profile where each player adopts a **rational** strategy
    * rational: maximizes its expected utility

Iterated elimination of strictly dominated strategies
* Start from the perspective of a column player
    * look at each row, pick the column of dominant action
    * if all dominant actions are in the same column, keep only that column
* If only one action left for each player, we found a *solution*

#### Nash equilibrium

* Dominant strategy equilibrium: each player has a dominant strategy
* Equilibrium: no player is willing to switch strategy, **given other players stick to their current strategy**
    * an equilibrium is a **local** optimum
    * test of equilibrium: fix all except on player's strategy, see if changing his strategy improves his expected reward
    * also called **Nash equilibrium**
* Domination
    * $s$ strictly dominates $s'$ if outcome for $s$ is better than $s'$ for **every** choice of strategy of other players
        * if iterated elimination of strictly dominated strategies end up with one strategy for each player, then the equilibrium is unique
        * order of elimination doesn't matter
    * $s$ weakly dominates $s'$ if outcome for $s$ is better than $s'$ in at least one strategy profile, and no worse in any other
        * elimination also find an equilibrium, but may not be unique
* Pareto optimal
    * Pareto optimal: no other outcomes that **all** players would prefer
    * Pareto dominated: all players would prefer the other outcome

#### Find pure strategy Nash equilibrium

* For each column, mark cell if it's best for the row player
* For each row, mark cell if it's best for the column player
* Cells with two marks are **pure** Nash equilibriums
    * every game has at least one Pareto optimal solution, may have multiple
    * but pure strategy Nash equilibrium may not exist
    * but mixed strategy Nash equilibrium always exists

#### Finding mixed strategies

* the algorithm does not always work - if there's no mixture where the player becomes indifferent (i.e. one action dominates the other choice), then the algorithm doesn't work
* most likely no efficient algorithm for finding a Nash equilibrium in general

#### Two player zero-sum games and Minimax theorem

Let $x=(p,1-p)$ and $y=(q,1-q)$ be the probability vectors of row players and column players, let the payoff matrix (with only row player's, or max player's payoff) be $A$, then the value of playing strategy profile $x,y$ is $x^TAy$

**Minimax theorem**: $\max_x \min_y x^TAy = \min_y \max_x x^TAy = v$ where $v$ is the value of the game (payoff for the max player)
* maximin strategy for max player, and minimax strategy for min player gives a Nash equilibrium of the game
* who goes first doesn't matter
* if the first player plays the maximin/minimax mixed strategy, the second player can play a pure strategy to get $v$ and $v$ is optimal for the second player

To find the maximin strategy
* because the min player can play pure strategy, then $\max_x \min_j \sum_{i=1}^m a_{ij}x_i$ subject to $\sum x_i=1$ and $x_i \geq 0$
    * $\sum_{i=1}^m a_{ij}x_i$ means given max player's strategy $x$, if min player plays action $j$, what's the expected outcome
* Solve the linear programming problem:
    * goal: $\max v$
    * constraint: $v \leq \sum_{i=1}^m a_{ij}x_i$, $\sum x_i=1$ and $x_i \geq 0$

#### Complexity of finding Nash equilibrium

* Every finite game has **at least one** Nash equilibrium
    * for two player zero sum game can be found efficiently
* General problem is hard

### Sequential games

* Extensive form: a game tree
    * terminal nodes: payoffs for each player
    * non-terminal nodes: action node for one player
    * model uncertainty: an extra player *chance* that takes random actions
    * model imperfect information/partial observability: **information sets**, a set of nodes that belong to the same player with the same observation history
        * simultaneous move = partial observability
        * perfect information = every node is an information set by itself
        * strategy now becomes assigning an action to each information set
* Subgame
* Subgame perfect equilibrium: a Nash equilibrium where every subgame is also at equilibrium
    * a solution for an extensive form game should be subgame perfect equilibrium
    * solutions of a normal form game may not be subgame perfect

### Repeated games

Repeated game: the players face the same choice repeatedly, but each time with the history of previous choices

* Finite repeated game: by **backward induction**, strategy remains the same
    * cooperative equilibrium is impossible
* Modified version: each round there's 99% chance that the players will meet again
    * expected rounds: 100, since mean of Geometric(0.01) is $1/0.01=100$
    * neither player know if it's the last round
    * **perpetual punishment** strategy: both player cooperate and play refuse (i.e. Pareto optimal), but if any player play testify, the other player will play testify from then on (i.e. get back to the non Pareto optimal Nash equilibrium strategy profile)
        * expected payoff of perpetual cooperation: $\sum_{t=0}^\infty 0.99^t \cdot (-1)=-100$
        * if I testify once, the opponent will play testify forever, my expected payoff becomes $0 + \sum_{t=1}^\infty 0.99^t \cdot (-5)=-495$, which is not good
    * **tit-for-tat**: start with refuse, then copy the other player's previous move