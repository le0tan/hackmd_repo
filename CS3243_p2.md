---
title: CS3244 notes (second half)
tags: Module, CS3244
---

## Week 6: Regularization and validation

* Regularization: restrain the model by adding overfit penalty
* Validation: Reality check by peeking

### Regularization

* Pros:
    * constrain the model from fitting the noise
    * lower variance
* Cons:
    * maybe unable to fit signal $f$
    * higher bias
* Soft order constraint
    * give a **budget**: $\sum_{q=0}^Q \theta_q^2 \leq C$
    * e.g. linear regression: minimize $J_{train}=1/m \times ||X\theta -y||^2$ **subject to $\theta^T\theta \leq C$**
* Soft order constraint can be also written as (by Lagrange multiplier): minimize $J_{train}(\theta) + \frac{\lambda}{m}\theta^T\theta$
    * $\theta = 0$ - overfitting
    * $\theta \rightarrow \infty$ - $h(x) \rightarrow$ a constant function (underfitting)

### Validation

Validation - estimate the test cost

* one test point: $E[j]=J_{test}(h_\theta), Var[j] = \sigma^2$ - high variance
* validation set of $K$ points: $E[j]=J_{test}(h_\theta), Var[j] = \sigma^2 / K$
    * $J_{val}(h) = J_{test}(h) \pm O(\frac{1}{\sqrt{K}})$
    * small $K$ - bad estimate
    * large $K$ - high test cost (not enough training data)
    * ![](https://i.imgur.com/ezvMSGC.png)
![](https://i.imgur.com/6m06XRe.png)

Fix number of total data points, consider the validation cost

![](https://i.imgur.com/GY83Z0j.png)

* with larger $K$, expected validation cost **increases** with fewer training data
* with small $K$, variance in validation cost is high because not enough validation points
* with large $K$, variance in validation cost is high because not enough training data to train the model
* variance in validation cost first decrease then increase

### Cleaniness of validation

* optimistic bias: estimated test cost from validation is smaller than actual test cost
* pessimistic bias: ...
* if we **pick** the model with smaller validation cost, most likely that we're having an optimistic bias

![](https://i.imgur.com/NZ804qj.png)

* red - test cost; blue - validation cost
* both go up because with less training data, model performs worse
* curves get closer because more validation data = more accurate estimate

Data contamination (=optimistic bias in estimating $J_{test}$)
* training set: totally contaminated
* validation set: slightly ~ (used to set hyperparameters, not directly tuning the model)
* test set: totally clean

### Normal solution for regularized linear regression

Problem: minimize $J_{train} = 1/m (X\theta-y)^T(X\theta-y)$ subject to $\theta^T\theta\leq C$

![](https://i.imgur.com/RTGSZUc.png)


* red circle: boundary of $\theta^T\theta = C$, $\theta$ inside the circle is within the constraint
* blue oval: $J_{train}$'s contour line
* $\theta_{lin}$ - (unregularized) linear regression result
* observation: the closer to $\theta_{lin}$, the smaller $J_{train}$
    * therefore the final solution would be the tangent point between red circle and the smallest blue oval
    * at this point, tangent is the same on both circle and oval

$\nabla J_{train}(\theta_{reg}) \propto -\theta_{reg}$ - as shown in the picture, $\nabla J_{train}$ has the opposite direction as normal vector, which happens to be $\theta{reg}$

Let $\nabla J_{train}(\theta_{reg}) = -2\frac{\lambda}{m}\theta_{reg}$, then as $C$ (the budget) increases, $\lambda$ decreases.

After some algebraic stuff, we have normal equation for regularized linear regression: $$\theta_{reg} = (X^TX + \lambda I)^{-1}X^Ty$$ Only difference is additional $\lambda I$ term in the inverse.

### Regularization variants

![](https://i.imgur.com/hmKgF69.png)

![](https://i.imgur.com/sJkPHlz.png)

* How to choose regularizer $\Omega$
    * if we know what target function looks like, we can let $\Omega$ to constraint $\mathcal{H}$ from looking different from $f$ - but that's impossible since we don't know what target function looks like
    * instead, we prefer **smoother** hypotheses to avoid fitting high-frequency/stochastic and deterministic noises
    * slightly higher bias, but lower variance
* How to choose $\lambda$
    * regularization

### Cross validation

* Leave one out cross validation (LOOCV)
    * 1 point for validation, $m-1$ points for training
        * do so for $m$ times, have $m$ hypotheses and $m$ validation costs
        * let the $cv$-th learned hypothesis be $h_{cv}^-$
        * $j_cv = j_{val}(h_{cv}^-)=j(h_{cv}^-(x^{(cv)}), y^{(cv)})$ - if the point taken out is $(x^{(cv)}, y^{(cv)})$
        * cross validation cost: $J_{loocv} = 1/m \sum_{cv=1}^mj(h_{cv}^-(x^{(cv)}), y^{(cv)})$
        * caveat: these $m$ hypotheses will be **highly correlated**
        * bad: very expensive ($m$ times) when number of data points is large
* K-fold cross validation
    * leave $m/k$ points for validation instead of 1
    * recommend: 10-fold CV
* Summary
![](https://i.imgur.com/Bjas2DL.png)

### Regularizing with $l^p$ norms

* L1 encourages sparse solution
    * tend to make certain parameters small (forced to the corner of the boundary)
    * useful for feature selection
    * when: lots of feature may be not useful/noisy
* L2 can be used for **homogeneous data**
    * e.g. images where pixels are of equal importance

## Week 7: Reinforcement learning

// handwritten notes

## Week 8: NN

// handwritten notes

## Week 10: Decision tree and ensembles

### Decision tree

What is a decision tree

* Internal nodes are tests
    * usually only test one component
* A branch corresponds to a test result
    * discrete: possible attribute values
    * continuous: range of values
* Each leaf node assigns
    * a class label - decision tree
    * a real value - regression tree

Analyze decision tree

* If data has no noise, decision tree can have zero training cost
* But **overfitting, high variance**
* The good
    * interpretable
    * efficient in both training and testing
    * discards irrelevant features via information gain
* The bad
    * instability: high variance, susceptible to small fluctuations
    * hard decision boundaries
    * axis perpendicular decisions: doesn't capture interaction between features (can use feature engineering to mitigate this)

#### Decision tree learning

Make smaller trees to lower the variance
* Prefer important decisions at the top
    * how to pick important feature
    * how to discretize continuous features
* Prune complete trees
    * where to stop pruning?

![](https://i.imgur.com/QmVMI8Z.png)

* `Choose-Attribute`: picking features via **entropy**
    * lower entropy = more pure: $H(X)=-\sum p\log_2(p)$
    * remainder: sum of entropy of subsets $S_j$ after choosing the attribute $x_i$
        * $\text{remainder}(S, x_i)=\sum \frac{|S_j|}{|S|}H(S_j)$, weighted sum of subset entropy
    * information gain = reduction in entropy: 
        * $IG(S,x_i) = H(S) - \text{remainder}(S,x_i)$
* Discretizing
    * ![](https://i.imgur.com/vSNoejb.png)
* Pruning
    * Use validation: remove node and replace my **mode** of label $y$, if pruned tree performs better on validation set then prune the node

### Ensemble

Given many hypotheses, we can **blend models** by aggregating all these hypotheses

* Select the best performing one one validation data
* Uniform vote
* Weighted vote
* Combine the predictions conditionally: **decision tree**

### Bootstrapping and bagging

* Bootstrapping (a statistical tool)
    * re-samples from given dataset to simulate a new dataset
* Bagging: bootstrap aggregation
    * bootstrap sample $\tilde{D}_t$: re-sample $m$ examples from dataset uniformly with replacement, for data randomness
    * ![](https://i.imgur.com/O99zUir.png)
    * train $T$ models using different bootstrap samples, have a uniform vote among all generated hypotheses
    * bagging works well if algorithm is **sensitive to data randomness**
* ![](https://i.imgur.com/2MUvwWH.png)
* Out of bag
* OOB versus Validation
* Bootstrapping as a re-weighting process
    * we can consider making the bootstrap sample as assigning weights to all training data
    * set of hypotheses $h_T$ is diverse by minimizing **bootstrap weighted error**

Bagging reduces variance
* by averaging over many hypotheses trained over different bootstrap samples

Bagging doesn't reduce bias
* it still works on the whole spectrum of $\mathcal{H}$

### Boosting and AdaBoost

* Boosting: an ensemble meta-algorithm to create strong learners from weak (slightly better than random) learners
    * reduce bias by boosting
    * reduce variance by bagging
* AdaBoost: adaptive boosting
    * weighted ensemble $h(x^{(j)})=\text{sign}(\sum_{t=1}^T \alpha_t h_t(x^{(j)}))$
    * weight for classifier $h_t$: $\alpha_t=0.5\times \ln(\frac{1-\epsilon_t}{\epsilon_t})$ where $\epsilon_t$ is the error rate
        * ![](https://i.imgur.com/t9NtC8K.png)
    * weight for examples: $D_{t+1}(j) = 1/Z_t * D_t(j)\exp(-\alpha_t \times (y^{(j)}h_t(x^{(j)})))$
        * when classified correct, decrease weight
        * when classified wrong, increase weight
        * the better the classifier (larger absolute value of $\alpha_t$), the change in weight is more
    * weak learner: decision stump
        * ![](https://i.imgur.com/WsEEvfk.png)

### Random forest

![](https://i.imgur.com/OYRbTGC.png)

## Week 12: Ethics

* Moral and ethical analyses
    * data gathering: method of collecting data
    * data bias: can lead to model bias
    * privacy by association: individual's data can adversely affect self and others
    * platform leverage
    * confirmation bias: feedback loop
    * trolly problem: needs moral built in
    * societal use (e.g. predictive policing)
* Data sharing
    * transparency: about how the data is collected, how it will be used
    * clear provenance of data: know where dataset comes from
    * equitable and ethical access: prevent large company advantage
    * ensuring quality of data
* Bias
    * ![](https://i.imgur.com/ZV0BcGn.png)
    * ![](https://i.imgur.com/75OlWyI.png)
* Types of bias
    * reporting bias: frequency of reported event doesn't reflect real-world frequency
    * automation bias: favor automated system's generated result rather than non-automated
    * selection bias: data set's examples are chosen in a way that is not reflective of their real-world distribution
        * Coverage bias: Data is not selected in a representative fashion.
        * Non-response bias (or participation bias): Data ends up being unrepresentative due to participation gaps in the data-collection process.
        * Sampling bias: Proper randomization is not used during data collection.
    * group attribution bias: tendency to generalize what is true of individuals to an entire group to which they belong
        * in-group bias: assume members within the group share similar features
        * out-group homogeneity bias
    * implicit bias: when assumptions are made based on one's own mental models and personal experiences that do not necessarily apply more generally
        * e.g. confirmation bias
* Evaluating bias
    * Metrics calculated against an entire set don't always give an accurate picture of how fair the model is.