---
title: CS3244 notes (first half)
tags: Module, CS3244
---

## Week 1

### Three criteria of ML
* A pattern exists;
* It's difficult to pin down formally (mathematically);
* We have data for it.

Absolutely essential: We have data for it.

### Notations

| Notation                  	| Meaning   	|
|----------------------------	|---------	|
| $\mathbf{x}$                | an input example (vector) |
| $\mathbf{X}$                | a stacked set of inputs of form $\mathbf{x}$ |
| $\mathbf{y}$                | set of (aligned) outputs 	|
| $n$                     	  | number of features 	|
| $m$                     	  | number of samples	|
|$\mathbf{x}_i$| $i^{th}$ feature of input $\mathbf{x}$ |
|$\mathbf{x}^{(j)}$|$j^{th}$ input or instance |

We will also use $\mathcal{X}$ denote the space of input values, and $\mathcal{Y}$ to denote the space of output values. 

### Paradigms of ML

* Supervised
    * for each input instance, there is a corresponding answer (a.k.a. label)
* Unsupervised
    * when input instances do not come with a label
* Reinforcement
    * problem instances do not have labels, but instead are arrayed in a series of states that yield a reward (or penalty)
    * Each part of the sequence in RL can be thought of as containing two logical parts: a state and an action.

### Five tribes of ML

![](https://i.imgur.com/AuHzNxf.png)


## Week 2

### Concept learning

Concept $\equiv$ a boolean-valued function $f$ over a set of instances $\mathbb{X}$ with each instance $\mathbb{x}$ comprising of **discrete** input attributes.

Concept learning is, supervised learning, white-box model

* *Satisfy*: an input instance $x \in \mathcal{X}$ **satisfies** a hypothesis $h \in \mathcal{H}$ iff $h(x)=1$ (hypothesis $h$ classifies $x$ as a +ve example)
* *Consistent*: a hypothesis $h$ is **consistent** with a set of training examples $D$ iff $h(x)=f(x)$ for all feature-label pair $<x,f(x)>$ in $D$
* *Conjunctive hypotheses*: $\mathcal{H}$ is the space of all conjunctions of constraints (any $h\in \mathcal{H}$ is a conjunction of several constraints)
* *Inductive learning assumption*: any hypothesis found to approximate the target function well over **a sufficiently large set of training examples** will also approximate the target function well over other **unobserved (test) examples**

### Feasibility of learning

If there are $k$ binary attributes, there would be $2^k$ possible inputs, and $2^{2^k}$ possible concepts, and we need training examples for every possible $x\in \mathcal{X}$ to converge to the target concept (unable to **generalize** beyond training examples).

* Inductive bias: (the prior assumption of $A$) is the only way to make learning feasible
    * a learner without a priori cannot classify unseen instances

### Concept learning as search

Infeasible due to number of possible concepts. Some important definitions for later discussion:

* $h_j$ is **more general or equal to** $h_k$ (i.e. $h_j \geq_g h_k$) iff $\forall x \in X: (h_k(x)=1) \rightarrow (h_j(x)=1)$
    * $\geq_g$ is a partial order, not a total order (not all hypotheses are comparable under $\geq_g$)
* $h_j$ is **more general than** $h_k$ iff $h_j \geq_g h_k$ and $h_k \ngeq_g h_j$
* $h_j$ is **more specific than** $h_k$ iff $h_k$ is more general than $h_j$

### Find-S

// TODO

### Candidate elimination

* Version space
    * Given hypothesis space $\mathcal{H}$ and training examples $\mathbb{D}$, the version  space $VS_{\mathcal{H},\mathbb{D}}$ is the subset of hypotheses from $\mathcal{H}$ that are consistent with $\mathbb{D}$: $VS_{\mathcal{H},\mathbb{D}}=\{h \in \mathcal{H} | h \text{  is consistent with  } \mathbb{D}\}$
    * If target function $f\in \mathcal{H}$, then a large enough dataset can reduce version space to $f$ itself
    * If $D$ is insufficient, then $VS_{\mathcal{H},\mathbb{D}}$ represents the uncertainty of what the target concept is
    * $VS_{\mathcal{H},\mathbb{D}}$ contains all **consistent** hypotheses, including the maximally specific hypothesis (found by Find-S)
* Version space representation theorem
    * ![](https://i.imgur.com/5t3XNjb.png)
* Candidate elimination
    * For each training example $x$, if $x$ is a +ve example
        * remove from $G$ any hypothesis inconsistent with $x$
        * for each $s\in S$ inconsistent with $x$
            * remove $s$ from $S$
            * add to $S$ all **minimal generalizations** $h$ of $s$ such that
                * $h$ is consistent with $x$
                * and some $g\in G$ is more general than $h$
            * remove from $S$ any hypothesis that's more general than any other hypothesis in $S$
    * for -ve examples, algorithm is similar
* Properties of candidate elimination
    * Error/noise in training data
        * $S$ and $G$ may be reduced to $\emptyset$ with sufficiently large data
    * ($f \notin H$) insufficiently expressive hypothesis representation
        * $S$ and $G$ may be reduced to $\emptyset$ with sufficiently large data
* Active learning in candidate elimination
    * active learning: request for training instance to best discriminate different hypotheses
    * strategy: choose instance that satisfy **exactly half** of hypotheses in the version space
    * complexity: each request reduces the VS by half, $O(log |VS_{\mathcal{H},\mathbb{D}}| )$
* Classify unobserved input instance
    * majority vote on every hypothesis in VS
    * assuming all hypotheses in $H$ are equally probable a priori


### Inductive bias

**Inductive bias** of $A$ is any minimal set of assertions $B$ such that for any target concept $f$ and corresponding training examples $D_f$, $\forall x \in \mathcal{X} (B\land D_f \land x) \implies (f(x) = A(x,D_f)$

Examples:
* Rote learner: no inductive bias
* Candidate elimination: $f\in \mathcal{H}$
* Find-S: $f\in \mathcal{H}$ and all instances are -ve, unless the opposite is entailed by its other knowledge

## Week 3

### Naive Bayes

**Bayes' Rule**: $P(y|X)=\frac{P(X|y)P(y)}{P(X)}$ where $P(y|X)$ is called **posterior**, $P(X|y)$ is **likelihood**, $P(y)$ is **prior**, $P(X)$ is **marginal**

* Posterior: how probable the instance is a positive case
* Likelihood: how probable is the data given $y$ is positive
* Prior: how probable $y$ is positive without seeing $X$ (the data)
* Marginal: how probable our data (the evidence) is under both classes of instances ($y$ is positive or negative)

**Chain Rule**: $P(x_1,\cdots,x_n)=P(x_1|x_2,\cdots,x_n)P(x_2|x_3,\cdots,x_n)\cdots P(x_n)$
Assume $x_1,\cdots,x_n$ are **conditionally independent** w.r.t $y$ (i.e. *naive assumption*), then $P(X|y)=P(x_1|y)P(x_2|y)\cdots P(x_n|y)$ - we reached **Naive Bayes**

**Naive Bayes**: $P(y|x) \approx P(x_1|y)P(x_2|y)\cdots P(x_n|y) \times P(y)$

Marginal is gone in this equation because it doesn't depend on the class of $y$ and the vector $x$ is given, so the marginal is a constant.

Problems:
* doesn't handle zeros (zero conditional probability) - solved by **Laplace smoothing**
* only works for categorical predictions, not regression
    * *(Optional)*: In the **sklearn** example we assume the distribution of features is Gaussian, and estimate the feature mean and standard deviation by the maximum likelihood method, and calculate $P(x_i|y)=\frac{1}{\sqrt{2\pi \sigma^2_y}}exp(-\frac{(x_i-\mu_y)^2}{2\sigma^2_y})$.

### Nearest neighbor

* An example of *analogizer*, a **parameter-free** algorithm
* Train: memorize the training data (no actual "training")
* Test: compute which training instance is closest to the test instance
* Distance metrics
    * L1 distance: $d(\vec{x},\vec{y}) = \sum_p |\vec{x}_p-\vec{y}_p|$ where $\vec{x}_p$ is the pth component of $\vec{x}$
    * L2 distance (Euclidean distance): $L2(x,y)=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+\cdots+(x_n+y_n)^2}$
    * defining distance metric to fit your data's **semantic meaning**, e.g. when there are incomparable categories in the feature definition, we should consider non L2 distance metric
* Drawbacks
    * requires a lot of time to apply (needs to cycle through all test (training???) data)
    * doesn't work well on high dimensions - **curse of dimensionality**

### Scaling

* Do you think scaling always helps? When would you use it?
    * Scaling is not necessary if the ranges for our parameters are similar. We would mostly use it when the ranges for our parameters are very different. Also, it isn't needed if our cost function isn't impacted by the magnitude of values.

### Cost functions

* Cost function: $J(h_\theta, f)$
* Point-wise cost: $j(h_\theta (x), f(x))$
* Training cost: $J_{train}=\frac{1}{m}\sum_{j=1}^m j(h_\theta (x^{(j)}), f(x^{(j)}))$
    * averaged instead of sum because otherwise datasets of difference sizes will have different costs
* Test cost: $\mathbb{E}_x[j(h_\theta (x), f(x))]$
* How to choose cost functions
    * choose one that fit the task specified by the user
    * when not possible, consider
        * plausible measures: e.g. squared error assumes Gaussian noise
        * convenient measures: e.g. closed form solution, convex optimization
    * false accepts and rejects may differ in badness

![](https://i.imgur.com/bD9RCq0.png)


### Laplace smoothing

* "add one smoothing"
* add 1 to each possible conditional outcome
* overcomes problems with **sparse data**, unseen observations
* "discounts" seen probabilities to account for unseen probabilities
* implies a uniform prior distribution assumption

### k-nearest neighbors

* Effect of $k$
    * smaller $k$: complex surface
    * larger $k$: smoother surface
    * when $k$ approaches $m-1$, regress to the majority class
    * $k=1$ maximizes performance on training data, but doesn't generalize well
* Hyperparameters
    * distance metric
    * $k$
    * normalization method
        * normalize each feature into $\mu=0$ and $\sigma=1$
* Set hyperparameters by
    * using prior knowledge of the data
    * using a reserved part of the training data: the validation set

### Data snooping

If a dataset has affected any step in the learning process, its ability to assess the outcome has been compromised.

### Noisy targets

* The target function may not be a function, as it may output different labels on identical sources - we need to think of its output as a conditional probability $P(y|x)$
* Data $(x,y)$ is now generated by joint distribution $P(x)P(y|x)$
* Noisy target: deterministic target function $f(x)=\mathbb{E}(y|x)$ + noise $(y-f(x))$
    * A deterministic target is a special case where $P(y|x)=0$ unless $y=f(x)$

### Curse of dimensionality

Problem with higher dimensions:
* \# of points needed to keep the same density grows exponentially
    * "sparsity" of higher dimensions
* distance between points is similar, making it hard to distinguish
    * most points end up far away "at the edge of the hypercube", and Euclidean distance becomes less meaningful
    * choose or design an appropriate distance metric

## Week 4

### Perceptron

$h_\theta (x)= \text{sign}(\theta_0 + \sum_{i=1}^n \theta_i x_i)$ where $\theta_0$, the bias weight, corresponds to the (negative of the) threshold

Simpler definition: introduce an artificial feature $x_0=1$: $h_\theta (x)=\text{sign}(\sum_{i=0}^n \theta_i x_i)=\text{sign}(\theta^Tx)$

### PLA: a simple learning algorithm

Start with whatever initialization of $\theta$

Pick a misclassified point $(\mathbf{x}^{(i)}, y^{(i)})$
* since it's misclassified, $sign(\theta^T\mathbf{x}^{(i)}) \neq y^{(i)}$
* update the weight vector: $\theta' \leftarrow \theta + y^{(i)}\mathbf{x}^{(i)}$
    * the update doesn't guarantee to move $\theta$ enough so that $\mathbf{x}^{(i)}$ is classified correctly

#### Pocket

Run PLA
* at each step, keep the best $J_{train}$ and corresponding $\theta$ so far
* ![](https://i.imgur.com/Xd89Dt2.png)


#### Proof of PLA

Theorem: if the data can be fit by a *linear separator*, then after some finite number of steps, PLA will find one.
(However, if the perceptron cannot fit the data, PLA may never terminate.)

Proof: https://colab.research.google.com/drive/1yE17zw0NNXPOdRIa27ai32p1v9G9psQm#scrollTo=qjzoZ9paIm8j

### Linear models

Three types of problem

![](https://i.imgur.com/gs813ZG.png)

The linear signal

![](https://i.imgur.com/jQaX6Yq.png)

#### Non-linear mapping

![](https://i.imgur.com/IKCV7zI.png)


### Linear regression

* Input: a vector $\mathbf{x}$ of features
* Output: $h_\theta(\mathbf{x})=\theta^T \mathbf{x}=\theta_0+\theta_1x_1+\cdots+\theta_nx_n$
* Cost function: squared error $(h_\theta (\mathbf{x}) - f(\mathbf{x}))^2$
* Training error: $J_{train}(h_\theta)=\frac{1}{m}\sum_{j=1}^m (h_\theta(x^{(j)}-y^{(j)})^2)$

#### Normal equation for linear regression

Expressing $J_{train}$ in matrix form, we have

![](https://i.imgur.com/JDQ7o6z.png)

$\mathbf{X}=\mathbb{R}^{m\times (n+1)}$ (with added bias term)

To minimize $J_{train}$, we take the derivative $\nabla J_{train}(\theta)=\vec{0}$

![](https://i.imgur.com/VnkHJGl.png)

So the close form of linear regression is $\theta = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty$ where $(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}$ is called the **pseudo inverse** of X.

Problems:
* Computing pseudo inverse is hard: since $\mathbf{X}=\mathbb{R}^{m\times (n+1)}$, $\mathbf{X}^T\mathbf{X}$ is $(n+1)\times (n+1)$, taking the inverse is $O(n^3)$
* $\mathbf{X}^T\mathbf{X}$ may not be invertible due to $\mathbf{X}$ being not full rank

#### Linear regression for classification

* Good initial weights for classification, but not good result weights
* Because of MSE, there is higher penalty for misclassified points far away from the decision boundary


### Logistic regression

* Logistic/Sigmoid function $g(s)=\frac{e^s}{1+e^s}$
    * ![](https://i.imgur.com/O6ibZLt.png)
* $h_\theta(x)=g(s)$ is interpreted as a probability
* Question: labels are binary, yet our outputs are probabilities
    * labels are "events", we can't measure a probability, we can only observe the occurances of events and **infer** the probability
    * the underlying target function is a noisy target $P(y|x)=\begin{cases}f(x), \text{for}\,\,y=+1\\1-f(x), \text{for}\,\,y=-1\end{cases}$
    * now the output of our logistic regression is consistent with the output of our target function: learn $h_\theta(x)=g(\theta^Tx)\approx f(x)$
* Cost function: $J_{train}(\theta)=\frac{1}{m}\sum_{j=1}^m \ln (1+exp({-y^{(j)}\theta^Tx^{(j)}}))$
    * $y^{(j)}$ is the ground truth
    * $\theta^Tx^{(j)}$ is the signal
    * only iterative solution

### Gradient descent


$\theta(1)=\theta(0) + \eta \nu$ where $\eta$ is the fixed step size, $\nu$ is the direction.

#### Formula for the direction $\nu$

$\Delta J_{train} = J_{train}(\theta(t+1))-J_{train}(\theta(t))$
$=J_{train}(\theta(t)+\eta \nu)-J_{train}(\theta(t))$
$=\eta \nabla J_{train}(\theta(t)^T)\nu + O(\eta^2)$ by Taylor approximation: $f(x_1)=f(x_0)+\nabla f(x_0)(x_1-x_0)+O(||x_1-x_0||^2)$
$\geq \eta ||\nabla J_{train}(\theta(t))||$

Therefore we let $\nu$ take the direction of $-\nabla J_{train}(\theta(t))$. In specific, we let $\nu$ be a unit vector: $\nu = -\frac{J_{train}(\theta(t))}{||J_{train}(\theta(t))||}$

#### Varying step size

* $\eta$ should increase with the slope
* Let $\Delta \theta = -\alpha \nabla J_{train}(\theta(t))$
    * fixed learning **rate** $\alpha$ instead of **step** $\eta$

#### Termination condition

Natural but not ideal: gradient $\lt$ threshold, it would get stop in the flat regions: ![](https://i.imgur.com/wIoUoq4.png)

Possible alternatives:
* error change is small
* error is small
* max number of iterations has reached

#### Logistic regression with GD

![](https://i.imgur.com/irAXiLp.png)


### Stochastic gradient descent (SGD)

* The GD mentioned before is **batch** gradient descent
    * sum **all** point-wise cost, average them
    * compute derivative and update $\theta$
* SGD - PLA + GD
    * considers the error on only **one** data point
    * "average" direction (the expected value of the direction taken by SGD) is the same as BGD
* Benefits of SGD
    * cheaper computation: only 1 cost to computer instead of m
        * Note that SGD may take more steps to terminate than BGD, however the total computation time may still be much less
    * stochastic: helps to escape local minima
    * simple
* Choice of learning rate $\alpha$
    * usually 0.1 works

### Cross entropy cost function

**Likelihood**: if $h_\theta = f$, how likely is it to obtain output $y$ from input $x$? 
Likelihood of $\mathbf{X}=(x^{(1)}, y^{(1)}),\cdots,(x^{(m)},y^{(m)})$ is $P(y^{(1)},\cdots,y^{(m)} | x^{(1)}, \cdots, x^{(m)})$
Suppose all samples in $\mathbf{X}$ are i.i.d, then $$P(y^{(1)},\cdots,y^{(m)} | x^{(1)}, \cdots, x^{(m)})=\Pi_{j=1}^m P(y^{(j)}|x^{(j)})$$

*rest refer to hand-written notes*

## Week 5

### Bias and variance

* Key: match the model complexity of the **data resources**, not to the target complexity.
* Bias and variance
    * *hand-written notes*
* Overfitting
    * fit the noise, useless and harmful
* Overfit measure
    * \# of data points ↑ -> Overfitting ↓
    * Stochastic noise ↑ -> Overfitting ↑
    * Deterministic noise ↑ -> Overfitting ↑
        * increasing the hypothesis complexity would reduce the deterministic noise (since the hypothesis can fit the training data better), but overfitting still increases (tutorial 4)
* Noises
    * Stochastic noise: data error
        * fluctuations that we cannot model
    * Deterministic: model error
        * the part of $f$ we lack the capacity to model
    * ![](https://i.imgur.com/MM0B2aH.png)
* Noise and bias/variance
    * *hand-written* notes



### Support vector machine

// hand-written notes



